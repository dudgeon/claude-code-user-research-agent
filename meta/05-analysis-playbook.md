# Research Analysis Playbook

## Overview

This playbook describes how to analyze the session research logs generated by the Trailblazers pilot. The analysis itself should be agent-assisted — you're eating your own cooking.

The analysis cadence follows the pilot timeline, with each round building on the previous.

---

## Data Collection

### Log Aggregation

Session logs accumulate in each participant's `.claude/research-logs/` directory. Collection options (choose based on infrastructure):

**Option A: Shared Git Repository**
Create a `trailblazers-research` repo. Each participant has a named subdirectory. They push (or a script pushes) their logs periodically. This is the most agent-friendly option — the entire corpus is accessible as a file tree.

**Option B: Scheduled Collection Script**
A lightweight script (bash or Python) runs on each participant's machine, copies new logs from `.claude/research-logs/` to a shared location (S3 bucket, network drive, or shared repo). Can be triggered manually or via cron.

**Option C: Manual Upload**
Participants upload logs to a shared folder (Google Drive, Box, internal share) at defined intervals. Lowest technical barrier, highest friction, least reliable.

**Recommendation**: Option A if participants are comfortable with git (which is itself a maturity indicator). Option B as fallback. Option C only if absolutely necessary.

### Log Inventory

Before each analysis round, generate an inventory:

```
Total logs collected: N
Logs per participant: [distribution]
Date range covered: [start] to [end]
Average logs per week per participant: N
Participants with zero logs this period: [list]
```

Zero-log participants are important data — they may indicate disengagement, friction, or workflow patterns where sessions don't have clear endpoints.

---

## Analysis Rounds

### Round 1: Week 2 — Early Signals (Lightweight)

**Goal**: Identify immediate friction points and quick wins. Catch any pilot participants who are stuck.

**Method**: Read through all logs. No quantitative analysis needed yet. Look for:

1. **Common friction points**: What are the top 3-5 things PMs are struggling with? Group by category (environment, delegation, tool integration, etc.)
2. **Surprise use cases**: Any tasks or approaches you didn't anticipate?
3. **Stuck participants**: Anyone showing zero progression or persistent Level 1 across all dimensions?
4. **Quick interventions**: What could you do this week to unblock the most common issues?

**Output**: A brief (1 page) memo with "Top 5 Things We're Seeing" and any immediate actions.

### Round 2: Week 4 — Pattern Analysis

**Goal**: Identify adoption archetypes and validate/refine the maturity model.

**Method**: Systematically code all logs and look for patterns.

**Quantitative analysis** (best done by an agent processing the log files):

- Task category distribution across all sessions (what are PMs actually using agents for?)
- Maturity dimension averages and distributions at Week 4
- Maturity progression: for each participant, compare earliest and most recent maturity assessments
- Session frequency and duration trends
- Configuration activity rates (what % of participants have modified CLAUDE.md? Created skills?)
- Integration patterns (what % of sessions involve copy-paste to/from legacy tools?)

**Qualitative analysis** (agent-assisted, human-validated):

- **Adoption archetypes**: Do participants cluster into recognizable groups? Hypothesized archetypes:
  - *The Drafter*: Uses agent primarily for content generation. High Task Breadth (Level 2-3), low Configuration Maturity.
  - *The Explorer*: Experiments widely, tries many task types, but doesn't go deep. High breadth, low integration.
  - *The Builder*: Invests in configuration, creates skills and commands. May have fewer total sessions but higher sophistication.
  - *The Integrator*: Focused on connecting agent work to team workflows. High Integration Depth, moderate other dimensions.
  - *The Skeptic*: Low engagement, persistent friction, may have reverted to legacy tools.
- **Maturity model validation**: Does the model capture the distinctions that matter? Are there dimensions that should be split, merged, or redefined? Are the level descriptions accurate?
- **Friction taxonomy**: Categorize all friction observations into a structured taxonomy. Which frictions are environmental (tooling), cognitive (mental model), social (team dynamics), or organizational (process/policy)?

**Output**: Full analysis report with data visualizations, archetype descriptions, and maturity model refinements.

### Round 3: Week 6 — Strategy Synthesis

**Goal**: Generate actionable recommendations for the scaling strategy.

**Method**: Synthesize Round 1 and Round 2 findings into specific recommendations.

For each recommendation, provide:
- **The insight**: What does the data show?
- **The implication**: What does this mean for scaling?
- **The recommendation**: What specifically should we do?
- **The evidence**: Which logs/patterns support this?

**Key questions to answer**:

1. **Training curriculum**: Given the friction taxonomy and adoption archetypes, what training modules are needed? In what sequence? With what prerequisites?

2. **Starter kit design**: Based on configuration patterns and skill utilization data:
   - What should be in the default `CLAUDE.md` for a new PM?
   - Which skills should be pre-loaded?
   - What project structure should be templated?

3. **Tool and environment decisions**: Based on environment fluency data:
   - Do PMs need a more accessible interface layer on top of Claude Code?
   - Is a specific editor (VS Code, Cursor) reducing friction significantly?
   - What's the minimum viable git knowledge PMs need?

4. **Integration playbooks**: Based on integration depth data:
   - What are the most effective patterns for connecting agent work to Jira, Google Docs, Slack?
   - Are there integration approaches that should be standardized?
   - What role should the PM play in SWE-facing agent workflows?

5. **Adoption readiness criteria**: Based on progression data:
   - What pre-pilot characteristics predict successful adoption?
   - What's the minimum support structure needed for a new cohort?
   - How long does it take to reach productive autonomy?

**Output**: Strategy recommendations document with specific, implementable actions.

### Round 4: Week 8 — Pilot Retrospective

**Goal**: Final comprehensive analysis. Validate strategy recommendations. Package learnings.

**Method**: Full longitudinal analysis of the complete log corpus.

- Participant progression timelines (maturity profiles at Weeks 1, 4, 8)
- Adoption curve modeling (time to key milestones by archetype)
- Final friction and success taxonomies
- Validated maturity model (revised based on pilot data)
- Comparison of initial predictions vs. actual outcomes

**Output**: Pilot retrospective report suitable for sharing with leadership. Finalized scaling strategy.

---

## Analysis Techniques

### Agent-Assisted Log Processing

The log corpus is itself a perfect agent task. Use Claude Code to:

1. **Parse and aggregate**: Read all logs from the shared directory. Extract structured data (task categories, maturity scores, friction observations) into a summary dataset.

2. **Pattern detection**: Ask the agent to identify recurring themes, clusters, and outliers across the corpus.

3. **Progression tracking**: For each participant, create a timeline of maturity assessments. Identify acceleration points and plateaus.

4. **Cross-referencing**: Correlate friction points with maturity levels. Do certain frictions appear consistently at specific maturity transitions?

### Useful Agent Prompts for Analysis

```
Read all session logs in [directory]. For each log, extract:
- Participant ID (from directory name)
- Date
- Task categories (from the checklist)
- Maturity scores (from the assessment table)
- All friction observations
- All win observations

Output a single Markdown table with one row per session.
```

```
Analyze the friction observations across all logs. Group them into categories.
For each category, report:
- Frequency (how many sessions mention this friction)
- Typical maturity level when this friction appears
- Whether it tends to resolve over time or persist
- Suggested intervention
```

```
For each participant, create a maturity progression summary showing their
assessed levels at their earliest and most recent sessions, noting which
dimensions showed the most and least growth.
```

### Visualization

For reports and presentations, generate:
- Radar charts of maturity profiles (per participant and cohort average)
- Heat maps of task category frequency over time
- Progression line charts per dimension
- Friction category distribution (bar chart)
- Session frequency histograms

---

## Outputs and Audiences

| Output | Audience | Cadence | Format |
|---|---|---|---|
| Quick signals memo | Research team, pilot leads | Week 2 | 1-page Markdown/email |
| Pattern analysis report | Research team, program leadership | Week 4 | Full report with data |
| Strategy recommendations | PM leadership, training team, tool selection committee | Week 6 | Structured recommendations doc |
| Pilot retrospective | All stakeholders, future cohort planners | Week 8 | Presentation + full report |
| Maturity model v2 | All future cohorts | Week 8 | Updated model document |
| Starter kit v1 | All future PMs | Week 6-8 | CLAUDE.md template + skill pack |

---

## Meta-Note: Dogfooding

This analysis workflow should itself be performed using Claude Code. The research corpus is a directory of Markdown files — exactly the kind of structured text that agents excel at processing. If you find yourself manually reading and tallying logs, you're not practicing what you preach. Use the agent. Build skills for the analysis tasks. Demonstrate the value of the approach by using it on its own data.
